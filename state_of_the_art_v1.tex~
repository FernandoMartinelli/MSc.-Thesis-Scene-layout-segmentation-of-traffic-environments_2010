\chapter{State of the art} \label{chap:state_of_the_art}

The problem of image segmentation has been focus, already for some decades, of countless image processing researchers around the globe. Although the problem itself is old, the solution to many segmentation tasks remains, still today, under active investigation---in particular for image segmentation applied to highly complex real-world scenes (e.g. traffic scenes). This chapter describes some of the techniques for image segmentation that have been applied in related areas to the one investigated in this thesis project.

\section{Features for image segmentation} \label{sec:features_for_segmentation}

\subsection{Spatial prior knowledge} \label{subsec:location_features}

One of the simplest but useful cues that may be explored when segmenting images in a supervised fashion is the location information of objects in the scene. For many object classes, there is an important correlation between the label to which a region in an image belongs and its location on the image. For instance, the fact that the road is mostly at the lower part of pictures could be helpful for its segmentation. The same applies for the segmentation of the sky, which is normally at the upper part of an image. Many similar examples can be mentioned---like buildings being usually on the sides of the image---which makes this feature powerful, despite its simplicity.

\subsection{Coarse 3D cues} \label{subsec:3d_features}

Different regions in an image have often different depths. Therefore, if available, the information of how far each point in the image was from the camera when the image was acquired can be very useful for segmentation purposes. Since individual images do not carry any depth information, 3D cues can only be explored in specific cases where one can either measure or infer how far the objects in an image are. 
If the use of radars or equipment that directly measure distance is to be discarded, 3D information can be inferred by using a stereo camera set or, in the case of a single camera, by using structure-from-motion techniques~\cite{hartley:reconstruction_techniques}. When dealing with images taken from an ordinary video frame, structure-from-motion techniques must be applied.

Figure~\ref{fig:point_cloud}, extracted from the work of Brostow et al.~\cite{brostow:structure_from_motion}, shows how accurate the segmentation of road scenes can get only by using reconstructed 3D point clouds.

\begin{figure}
 \centering
{\includegraphics[height=3cm]{figures/point_cloud.eps}}
\caption[3D features]{The algorithm proposed by Brostow et al. uses 3D point clouds estimated from videos sequences and performs, using motion and structure features, a very satisfactory 11-class semantic segmentation.}
\label{fig:point_cloud}
\end{figure}

Brostow et al. based their work on the following features, which can be extracted from the sparse 3D point cloud:
\begin{itemize}
 \item Height above the ground;
 \item Shortest distance from car's path;
 \item Surface orientation of groups of points;
 \item Residual error, which is a measure of how objects in the scene move in respect to the world;
 \item Density of points.
\end{itemize}

\subsection{Gradient-based edges}

When ones thinks of image segmentation, it is natural to expect that the label boundaries correspond to strong edges on the image being segmented. For example, the image of a blue car on a city street will have rather strong edges where, in a perfect labeling, the boundaries between the labels `car' and `street' are located. Some methods, like, for example, active contour snakes~\cite{Kass88snakes:active}, explore gradient-based edge information for segmentation. Figure~\ref{fig:edge_potential} shows an example picture of Lena and its gradient. The white pixels have a greater probability of being located on boundaries between labels in a segmentation. 

\begin{figure}
\centering{
\begin{tabular}{cc}
\includegraphics[scale=0.5,bb=0 0 256 256]{figures/Lenna.png} &
\includegraphics[scale=0.5,bb=0 0 256 256]{figures/edge_potential_lenna.png}\\
(a)&(b)
\end{tabular}
}
\caption[Gradient-based edges]{(a) Original grayscale image of Lena. (b) Edge image obtained by calculating the image gradients. Edge based segmentation methods explore the information in (b) to propose a meaningful segmentation of (a). Note how Lena's hat, face and shoulder could be quite well segmented only with this edge cue.}
\label{fig:edge_potential}
\end{figure}

Notice that although this is a very reasonable and useful cue, it can also turn out to be misleading. When dealing, for example, with shadowed scenes, very often there are stronger edges inside regions that belong to the same label than there are on the boundaries between labels. This is particularly challenging for real-world scenes such as the traffic scenes considered in this thesis project. The way this cue was explored in this project is explained in detail in Section~\ref{subsec:edge_potential}.

\subsection{Color distribution} \label{subsec:color_features}
Early methods, like~\cite{saber:color} tackle the problem of image segmentation by relying solely on color features, which can be modeled as histogram distributions or by Gaussian Mixture Models (GMMs). A Gaussian Mixture Model represents a probability distribution, $P(x)$, which is obtained by summing different Gaussian distributions:
\begin{equation}
P(x) = \sum_k P_k(x)
\end{equation}
where
\begin{equation}
P_k(x) = \mathcal{N}(x|\mu_k,\Sigma_k)
\end{equation}
$\mu_k,\Sigma_k$ being the mean and variance of the individual Gaussian distribution $k$.

The use of GMMs to model colors in images has also proven very efficient in binary segmentation problems, as shown by Rother et al~\cite{rother:grab_cut} with their GrabCut algorithm. In such problems, one wants to separate a foreground object from the background for image editing, object recognition and so on. When possible, user interaction can be very useful to refine the results by giving important feedback after the initial automatic segmentation (see Figure~\ref{fig:grab_cut}).

\begin{figure}
 \centering
{\includegraphics[scale=0.4,bb=0 0 709 594]{figures/grab_cut.png}}
\caption[Segmentation achieved by GrabCut using color GMMs and user interaction]{Segmentation achieved by GrabCut using color GMMs and user interaction.}
\label{fig:grab_cut}
\end{figure}

However, in most cases, either the number of images to segment is prohibitive or the real time nature of the segmentation task prevents any user interference at all. These both remarks are true in the field of traffic scenes segmentation for driver assistance. 

\subsection{Texture cues}

Along with color, texture information is often considered and can bring significant improvement to the segmentation accuracy, as in~\cite{feng:color_tex}, where graylevel texture features were combined to color ones. Nowadays, most if not all the research effort on segmentation also incorporates texture information. This can be extracted and modeled in two main ways:

\begin{enumerate}
\item \emph{Statistical Models,} which try to describe the statistical correlation between pixel colors within a restricted vicinity. Among such methods, co-occurrence matrices, have been successfully used, for instance, for seabed classification~\cite{petillot:seabed};

\item \emph{Filter bank convolution,} where the image is convolved with a carefully selected set of filter primitives, usually composed of Gaussians, Gaussian derivatives and Laplacians. A well known example, the Leung-Malik (LM) filter bank~\cite{malik:filter_bank}, is shown in Figure~\ref{fig:lm_filter}. It is interesting to mention that such filter banks have similarities with the receptive fields of neurons in the human visual cortex.
\end{enumerate}

\begin{figure}
 \centering
{\includegraphics[height=3cm]{figures/lm_filter_bank.eps}}
\caption[The Leung-Malik (LM) filter bank]{The LM filter bank has a mix of edge, bar and spot filters at multiple scales and orientations. It has a total of 48 filters---2 Gaussian derivative filters at 6 orientations and 3 scales, 8 Laplacian of Gaussian filters and 4 Gaussian filters.}
\label{fig:lm_filter}
\end{figure}

\subsection{Context features} 

Although color and texture may efficiently characterize image regions, they are far from enough for a high quality semantic segmentation if considered alone. For instance, even humans may not be able to tell apart, when looking only at a local patch of an image, a blue sky from the walls of a blue building. The key aspect of which humans naturally take advantage, and that allows them to unequivocally understand scenes, is the context. Even if one sees a building wall painted with exactly the same color as the sky, one just knows that that wall cannot be the sky because it is surrounded by windows. In the case of road scenes segmentation, typical spatial relationships between objects can be a very strong cue---for example, the fact that the car is always on the road and the sidewalk surrounds it.

With this in mind, computer vision researchers are now frequently looking beyond low-level features and are more interested in contextual issues~\cite{feng:color_tex, re:multiscale_CRF, kumar:DRF}. In Section~\ref{sec:TextonBoost}, an example of how context in images can be exploited for segmentation is described.

\section{Probabilistic framework for segmentation} 

The choice of image features, described in the previous section, is independent of the theoretical framework or machine learning technique applied for segmentation inference. One can choose the very same features as in~\cite{feng:color_tex}, where belief networks are used, and process them using Support Vector Machines, for example. In recent years, Conditional Random Fields (CRFs) have played an increasingly central role. CRFs have been introduced by Lafferty et al. in~\cite{lafferty:crf} and have ever since been systematically used in cutting-edge segmentation and classification approaches like TextonBoost~\cite{jamie:texton_boost}, image sequence segmentation~\cite{wang:dynamic_CRFs}, contextual analysis of textured scenes~\cite{turtinen:contextual_analysis} and traffic scene understanding~\cite{sturgess:road_scene}, to name a few. Conditional Random Fields are based on Markov Random Fields and offer practical advantages for image classification and segmentation. These advantages are explained in section~\ref{subsec:CRF} after the formal definition of Markov Random Fields is given. 

\subsection{Markov Random Fields} \label{subsec:MRF}

In the Markov Random Field theory, an image can be described by a lattice $\mathcal S$ composed of sites $i$, which can be thought of as the image pixels. The sites in $\mathcal S$ are related to one another via a neighborhood system, which is defined as ${\mathcal N}=\{{\mathcal N}_i, i \in \mathcal S\}$, where ${\mathcal N}_i$ is the set of sites neighbouring $i$. Additionally, $i \notin {\mathcal N}_i$ and $i \in {\mathcal N}_j \Longleftrightarrow j \in {\mathcal N}_i$. 

Let $y$ denote a labeling configuration of the lattice $\mathcal S$ belonging to the set of all possible labelings $\mathcal Y$. In the image segmentation context, $y$ can be seen as a labeling image, where each of the sites (or pixels) $i$ from the lattice $\mathcal S$ is assigned one label $y_i$ in the set of possible labels $\mathcal L = \{ l_1, l_2, l_3, \cdots, l_N\}$, which are the object classes. The pair $(\mathcal S, \mathcal N)$ can be referred to as a Random Field.

Moreover, $(\mathcal S, \mathcal N)$ is said to be a Markov Random Field (MRF) if and only if
\begin{equation}
P(y)>0, \forall y \in \mathcal Y \textrm{, and}
\end{equation}
\begin{equation}
P(y_i\vert y_{{\mathcal S}-\{i\}})=P(y_i\vert y_{{\mathcal N}_i}) 
\end{equation}

That means, firstly, that the probability of any defined label configuration must be greater than zero\footnote{This assumption is usually taken for convenience, as it, in practical terms, does not influence the problem.} and, secondly and most importantly, that the probability of a site assuming a given label just depends on its neighboring sites. The latter statement is also known as the Markov condition.

According to the Hammersley-Clifford theorem~\cite{besag:clifford_theorem}, an MRF like defined above can equivalently be characterized by a Gibbs distribution. Thus, the probability of a labeling $y$ can be written as
\begin{equation} \label{gibbs} P(y)=Z^{-1}\exp(-U(y)), 
\end{equation}
where
\begin{equation}Z=\sum_{y \in \mathcal Y}\exp(-U(y))
\end{equation}
is a normalizing constant called the partition function, and $U(y)$ is an energy function of the form
\begin{equation} \label{energy_u} 
U({\mathbf y})=\sum_{c \in \mathcal C}V_c({y}). 
\end{equation}
$\mathcal C$ is the set of all possible cliques and each clique $c$ has a clique potential $V_c(y)$ associated with it. A clique $c$ is defined as a subset of sites in $\mathcal S$ in which every pair of distinct sites are neighbours, with single-site cliques as a special case (see Figure~\ref{fig:cliques}). Due to the Markov condition, the value of $V_c(y)$ depends only on the local configuration of clique $c$.

\begin{figure}[htb]
\centering{
\begin{tabular}{ccc}
\includegraphics[height=3cm]{figures/cliques_neighboorhod.eps} &
\includegraphics[height=3cm]{figures/cliques_unary.eps} &
\includegraphics[height=3cm]{figures/cliques_binary.eps}\\
(a)&(b)&(c)
\end{tabular}
}
\caption[Clique layouts]{(a) Example of a 4-pixel neighborhood. (b) Possible unary clique layout. (c) Possible binary clique layouts.}
\label{fig:cliques}
\end{figure}

Now let us consider the observation $x_i$, for each site $i$, which is a state belonging to a set of possible states ${\mathcal W}=\{w_1, w_2, \cdots, w_n\}$. In this manner, we can represent the image we want to segment, where each pixel $i$ is assigned to one state of the set $\mathcal W$. If one thinks of a gray scale image with 8 bit-resolution, for example, the set of possible states for each site (or pixel) would be defined as $\mathcal W = \{ 0, 1, 2, \cdots, 255\}$. The segmentation problem then boils down to finding the labeling $y^*$ such that $P(y^*|x)$---the posterior probability of labeling $y^*$ given the observation $x$---is maximized. Bayes' theorem tells us that 
\begin{equation}
P(y|x) = P(x|y)P(y)/P(x)
\end{equation}
where $P(x)$ is a normalization factor, as $Z$ in Eq.~\ref{gibbs}, and plays no role in the maximization. Thanks to the Hammersley-Clifford theorem, one can greatly simplify this maximization problem by defining only locally the clique potential functions $V_c(x,y,\theta)$. How to choose the forms and parameters $\theta$ of the potential functions for a specific application is a major topic in MRF modeling and will be further discussed in Chapter 4.

\subsection{Conditional Random Fields} \label{subsec:CRF}

The main difference between MRFs and CRFs lies on the fact that MRFs are generative models, whereas CRFs are discriminative. That is, CRFs directly model the posterior distribution $P(y|x)$ while MRFs learn the underlying distributions $P(x|y)$ and $P(y)$, arriving at the posterior distribution by applying the Bayes theorem.

In other words, for MRFs, the learned state-label joint probability is represented as $P(y|x) = P(x|y)P(y)/P(x))$, where $x$ represents the observation and $y$ the corresponding labeling configuration.  However, for CRFs, it is not required to generate prior distributions over the labels $P(x|y)$ like for MRFs, as the a posteriori $P(y|x)$ is modeled directly. 

This directly modeled posterior probability is simpler to implement and usually sufficient for segmenting images. Hence, for the road scene segmentation and classification problem at hand, CRFs are advantageous in comparison to MRFs.  This is the main reason why they became so popular~\cite{jamie:texton_boost, wang:dynamic_CRFs, sturgess:road_scene}. 

\subsection{Energy minimization for label inference}

Finding the labeling $y^*$ that maximizes the a posteriori probability expressed in Eq.~\ref{gibbs} is equivalent to finding $y^*$ that minimizes the energy function in Eq.~\ref{energy_u}. An efficient way of finding a good approximation of the energy minimum of such functions, is the alpha-expansion graph-cut algorithm~\cite{boykov:graph_cut} which widely used along with MRFs and CRFs. The idea of the alpha-expansion algorithm is to reduce the problem of minimizing a function like $U(y)$ with \emph{multiple labels} to a sequence of \emph{binary} minimization problems. These sub-problems  are referred to as `alpha-expansions', and will be shortly described for completeness (for details see~\cite{boykov:graph_cut}). 

Suppose that we have a current image labeling $y$ and one randomly chosen label $\alpha \in \mathcal L = \{ l_1, l_2, l_3, \cdots, l_N\}$. In the alpha-expansion operation, each pixel $i$ makes a binary decision: it can either keep its old label $y_i$ or switch to label $\alpha$, provided that this change decreases the value of energy the function. For that, we introduce a binary vector $s \in \{0, 1\}^{M \times N}$ which indicates which pixels in the image (of size ${M \times N}$) keep their label and which switch to label $\alpha$. This defines the auxiliary configuration $y[s]$ as
\begin{equation} 
y_i[s] = 
\begin{cases} 
y_i,& \mbox{if }s_i=0 \\
\alpha,& \mbox{if }s_i=1 
\end{cases}
\end{equation}

This auxiliary configuration $y[s]$ transforms the function $U$ with multiple labels into a function of binary variables $U'(s) = U(y[s])$. If function $U$ is composed of attractive potentials, which could be seen as a kind of convex functions, the global minimum of this binary function~\footnote{Notice that this does not mean that the global minimum of the multi-label function is found.} is guaranteed to be found exactly using standard graph cuts~\cite{jamie:texton_boost}.

The expansion move algorithm starts with any initial configuration $y_0$, which could be set, for instance, taking, for each pixel, the label with maximum location prior probability\footnote{In the road scene segmentation case, for instance, pixels on top of the image could start with label `sky' and pixels at the bottom with label `road'. This is equivalent to exploring the features described in~\ref{subsec:location_features}} . It then computes optimal alpha-expansion moves for labels $\alpha$ in a random order, accepting the moves only if they decrease the energy function. The algorithm is guaranteed to converge, and its output is a strong local minimum, characterized by the property that no further alpha-expansion can decrease the value of function $U$.

\section{Example: TextonBoost} \label{sec:TextonBoost}

One CRF-based approach to image segmentation that is currently fundamental for state-of-the-art methods is TextonBoost~\cite{jamie:texton_boost}. In their research, Shotton et al. have used the Microsoft Research Cambridge (MSRC) database\footnote{The MSRC database can be downloaded at http://research.microsoft.com/vision/cambridge/recognition/}, which is composed of 591 photographs of the following 21 object classes: building, grass, tree, cow, sheep, sky, airplane, water, face, car, bicycle, flower, sign, bird, book, chair,
road, cat, dog, body, and boat. Approximately half of those pictures is picked for training, in a way that ensures proportional contributions from each class. Some results of their semantic segmentation on previously unseen images are shown in Figure~\ref{fig:textonboost_results}.

\begin{figure}[htb]
 \centering
{\includegraphics[width=11.5cm]{figures/textonboost_results.eps}}
\caption[Sample results of TextonBoost]{TextonBoost results extracted from~\cite{jamie:texton_boost}. Above, unseen test images. Below, segmentation using a color-coded labeling. Textual labels are superimposed for better visualization. }
\label{fig:textonboost_results}
\end{figure}

Since the algorithm implemented for the segmentation of road scenes in this master thesis has been mainly inspired by TextonBoost, a short description of the way it works is provided.

The inference framework used is a conditional random field (CRF) model~\cite{lafferty:crf}. The CRF learns, through the training of the parameters of the clique potentials, the conditional distribution over the possible labels given an input image. The use of a conditional random field allows the incorporation of texture, layout, color, location, and edge cues in a single, unified model. The energy function $U(y|x,\theta)$, which is the sum of all the clique potentials (see Eq.~\ref{energy_u}), is defined as:
\begin{equation} \label{eq:textonboost_potentials}
U(y|x,\theta) = 
\sum_i \Big[
\overbrace{\lambda(y_i,i;\theta_{\lambda})}^{location}+
\overbrace{\pi(y_i,x_i;\theta_{\pi})}^{color}+
\overbrace{\psi_i(y_i,\textbf{x};\theta_{\psi})}^{texture-layout} \Big]\\
+ \sum_{(i,j)\in \varepsilon}\overbrace{\phi(y_i, y_j, g_{ij}(x);\theta_{\phi})}^{edge}
\end{equation}
where $y$ is the labeling or segmentation and $x$ is a given image, $\varepsilon$ is the set of edges in a 4-connected neighborhood, $\theta = \{\theta_{\psi}, \theta_{\pi}, \theta_{\lambda}, \theta_{\phi}\}$ are the
model parameters, and $i$ and $j$ index pixels in the image, which correspond to sites in the lattice of the Conditional Random Field. Notice that the model consists of three \emph{unary} potentials, which depend only on one site $i$ in the lattice, and one \emph{pairwise} potential, depending on pairs of neighboring sites.

Each of the potentials is subsequently explained in a simplified way, for details please see~\cite{jamie:texton_boost}. 

\subsection{Potentials without context}
\subsubsection{Location potential}

The unary location potentials $\lambda(y_i,i;\theta_{\lambda})$ capture the correlation of the class label and the absolute location of the
pixel in the image. For the databases with which TextonBoost was tested, the location potentials had a rather low importance since the context of the pictures is very diverse. In the case of our road scene segmentation, which is a more structured environment, they have had significantly more relevance, as discussed in Chapter 5.

\subsubsection{Color potential}

In TextonBoost, the color distributions of object classes are represented as Gaussian
Mixture Models (see Section~\ref{subsec:color_features}) in CIELab color space
where the mixture coefficients depend on the
class label. The conditional probability of the
color $x$ of a pixel labeled with class $y$ is given by 
\begin{equation}
P(x|y) = \sum_k P(x|k)P(k|y)
\end{equation}
with color clusters (mixture components) $P(x|k)$. Notice that the clusters are shared between different classes, and that only the coeficients $P(k|y)$ depend on the class label. This makes the model more eficient to learn than a separate GMM for each class, which is important since TextonBoost takes into account a high number of classes.

\subsubsection{Edge potential} \label{subsec:edge_potential}

The pairwise edge potentials $\phi$ have the form of a contrast sensitive Potts model~\cite{boykov:graph_cut},
\begin{equation}
\phi(y_i, y_j, \textbf{g}_{ij}(\textbf{x});\theta_{\phi})=-\theta_{\phi}^T \textbf{g}_{ij}(\textbf{x})[y_i\ne y_j],
\end{equation}
with $[\cdot]$ the zero-one indicator function:
\begin{equation} 
[condition] = 
\begin{cases} 
1,& \mbox{if } condition\mbox{ is true}\\
0,& \mbox{otherwise} 
\end{cases}
\end{equation}
The edge feature $\textbf{g}_{ij}$ measures the difference in color between the neighboring pixels, as suggested by~\cite{rother:grab_cut},
\begin{equation}
\textbf{g}_{ij}= 
\begin{bmatrix}
  \exp(-\beta \left \Vert x_i-x_j \right \|^2\\
  1  
\end{bmatrix}
\end{equation}
where $x_i$ and $x_j$ are three-dimensional vectors representing the CIELab colors of pixels $i$ and $j$ respectively. Including the unit element allows a bias to be learned, to remove small, isolated regions\footnote{The unit element means that for every pair of pixels that have different labels a constant potential is added to the whole. This makes contiguous labels preferable when the energy function is minimized.}. The quantity $\beta$ is an image-dependent contrast term, and is set separately for each image to $(2\langle \left \Vert x_i-x_j \right \|^2  \rangle)^{-1}$, where $\langle \cdot \rangle$ denotes an average over the image. The two scalar constants that compose the parameter vector $\theta_{\phi}$ are appropriately set by hand.

\subsection{Texture-layout potential}\label{subsec:texture_layout_potential_state_of_the_art}

The texture-layout potential is the most important contribution of TextonBoost. It is based on a set of novel features which are introduced in~\cite{jamie:texton_boost} as texture-layout filters. These new features are capable of, at once, capturing the correlation between texture, spatial layout, and textural context in an image.

Here, we quickly describe how the texture-layout features are calculated and the boosting approach used to automatically select the best features and, thereby, learn the texture-layout potentials used in Eq.~\ref{eq:textonboost_potentials}.

\subsubsection{Image textonization}

As a first step, the images are represented by textons~\cite{malik:textons} in order to arrive at a compact representation of the vast range of possible appearances of objects or regions of interest\footnote{Textons have been proven effective in categorizing materials~\cite{varma:materials} as well as generic object classes~\cite{winn:object_classes}.}. The process of textonization is depicted in Figure~\ref{fig:textonization}, and proceeds as follows. At first, each of the training images is convolved with a 17-dimensional filter bank. The responses for all training pixels
are then whitened---so that they have zero mean and unit covariance---and clustered using a standard Euclidean-distance K-means clustering algorithm for dimension reduction. Finally, each pixel in each image is assigned to the nearest cluster center found with K-means, producing the texton map $T$, where pixel $i$ has value $T_i \in \{1, . . . ,K\}$.

\begin{figure}[htb]
 \centering
{\includegraphics[width=13cm]{figures/textonization.eps}}
\caption[The process of image textonization]{The process of image textonization, as proposed by~\cite{jamie:texton_boost}. All training images are convolved with a filter-bank. The filter responses are clustered using K-means. Finally, each pixel is assigned a texton index corresponding to the nearest cluster center to its filter response.}
\label{fig:textonization}
\end{figure}

\subsubsection{Texture-Layout Filters} 

The texture-layout filter is defined by a pair $(r, t)$ of an image region, $r$, and a texton $t$, as illustrated in Figure~\ref{fig:texture_layout_filter}. Region $r$ is relatively referenced to the pixel $i$ being classified and texton $t$ belongs to the texton map $T$. For efficiency reasons, only rectangular regions are implemented by TextonBoost, although any arbitrary region shape could be considered. A set $R$ of candidate rectangles is chosen at random, such that every rectangle lies inside a fixed bounding box.

The feature response at pixel $i$ of texture-layout filter $(r, t)$ is the proportion of pixels under the offset region $r + i$ that have been assigned texton $t$ in the textonization process,
\begin{equation}
v_{\left [ r,t \right ] }(i)= \frac{1}{\textnormal{area}(r)} \sum_{j \in(r+i)} \left [ T_j = t \right ].
\end{equation}

\begin{figure}[htb]
\centering{
\begin{tabular}{ccc}
\includegraphics[height=3cm]{figures/tex_lay_a.eps} &
\includegraphics[height=3cm]{figures/tex_lay_c.eps} &
\includegraphics[height=3cm]{figures/tex_lay_e.eps}\\
(a)&(c)&(e)\\
\includegraphics[height=3cm]{figures/tex_lay_b.eps} &
\includegraphics[height=3cm]{figures/tex_lay_d.eps} &
\includegraphics[height=3cm]{figures/tex_lay_f.eps}\\
(b)&(d)&(f)
\end{tabular}
}
\caption[Texture-layout filters]{Graphical explanation of texture-layout filters extracted from~\cite{jamie:texton_boost}. {\bf (a, b)} An image and its
corresponding texton map (colors represent texton indices). {\bf (c)} Texture-layout filters are defined relative to the point $i$ being classified (yellow cross). In this first example feature, region $r_1$ is combined with texton $t_1$ in blue. {\bf (d)} A second feature where region $r_2$ is combined with texton $t_2$ in green. {\bf (e)} The response $v_{\left [ r_1,t_1 \right ] }(i)$ of the first feature is calculated at three positions in the texton map (magnified). In
this example, $v_{\left [ r_1,t_1 \right ] }(i_1) \simeq 0$, $v_{\left [ r_1,t_1 \right ] }(i_2) \simeq 1$, and $v_{\left [ r_1,t_1 \right ] }(i_3) \simeq 1/2$. {\bf (f)} The second feature $(r_2, t_2)$, where $t_2$ corresponds to `grass', can learn that points $i$ (such as $i_4$) belonging to sheep regions tend to produce large values of $v_{\left [ r_2,t_2 \right ] }(i)$, and hence can exploit the contextual information that sheep pixels tend to be surrounded by grass pixels.
}
\label{fig:texture_layout_filter}
\end{figure}

Any part of the region $r+i$ that lies outside the image does not contribute to the feature response.

An efficient and elegant way to calculate the filter responses anywhere
over an image can be achieved with the use of integral images~\cite{viola:integral_images}. For each texton $t$ in the texton map $T$, a separate integral image $I^{(t)}$ is calculated. In this integral image, the value at pixel $i = (u_i,v_i)$ is defined as the number of pixels in the original image that have been assigned to texton $t$ in the rectangular region with top left corner at $(1,1)$ and bottom right corner at $(u_i,v_i)$:
\begin{equation}
I^{(t)}(i) = \sum_{j:(u_j \leq u_i) \& (v_j \leq v_i)} \left [ T_j = t \right ].
\end{equation}

The advantage of integral images is that they can later be used to compute the texture-layout
filter responses in constant time: if $I^{(t)}$ is the integral image for texton channel $t$ defined like above, then the feature response is computed as:
\begin{equation}
v_{\left [ r,t \right ] }(i)= \big( I^{(t)}(r_{br}) - I^{(t)}(r_{bl})
- I^{(t)}(r_{tr}) + I^{(t)}(r_{tl}) \big)/\textnormal{area}(r)
\end{equation}
where $r_{br}$, $r_{bl}$, $r_{tr}$ and $r_{tl}$ denote the bottom right, bottom left, top right and top left corners of rectangle $r$.

Texture-layout features are sufficiently general to allow for an automatic learning of layout and context information. Figure~\ref{fig:texture_layout_filter} illustrates how texture-layout filters are able to model textural context and layout.

\subsubsection{Boosting of texture-layout filters}

A Boosting algorithm iteratively selects the most discriminative
texture-layout filters $(r,t)$ as `weak learners' and combines them into a strong classifier used to derive the texture-layout potential in Eq.~\ref{eq:textonboost_potentials}. The boosting scheme used in TextonBoost shares each weak learner between a set of classes $C$, so that
a single weak learner classifies for several classes at once. According to the authors, this allows for classification with cost sub-linear in the number of classes, and leads to improved
generalization.

The strong classifier learned is the sum over the classification confidences $h_i^m(c)$ of $M$ weak learners
\begin{equation}
H(y_i,i) = \sum_{m=1}^M h_i^m(c)
\end{equation}

The confidence value $H(y_i,i)$ for pixel $i$ is then just multiplied by a negative constant---so that a positive confidence turns into a negative energy, which will be preferred in the energy minimization---to give the texture-layout potentials $\psi_i$ used in Eq.~\ref{eq:textonboost_potentials}:
\begin{equation}
\psi_i(y_i,\textbf{x};\theta_{\psi}) = - cte.H(y_i,i)
\end{equation}

Each weak learner is a decision stump based on the feature response $v_{\left [ r,t \right ] }(i)$ of the form
\begin{equation} \label{eq:weak_class_textonboost}
h_i[c] = 
\begin{cases} 
a \left [ v_{\left [ r,t \right ] }(i) > \theta \right ] +b,& \mbox{if }c \in C \\
k^c,& \mbox{otherwise,} 
\end{cases}
\end{equation}
with parameters $(a, b, k^c, \theta, C, r, t)$. The region $r$ and texton index $t$ together specify the texture-layout filter feature, and $v_{\left [ r,t \right ] }(i)$ denotes the corresponding feature response at position $i$. For the classes that 
share this feature, that is, $(c \in C)$, the weak learner gives $h_i(c) \in \{a + b, b\}$ depending on whether $v_{\left [ r,t \right ] }(i)$ is, respectively, greater or lower than a threshold $\theta$. For classes not sharing the feature $(c \notin C)$, the constant $k^c$ ensures that unequal numbers of training examples of each class do not adversely affect the learning procedure.

In order to choose the weak classifiers, TextonBoost uses the standard boosting algorithm introduced by Schapire et al. in~\cite{freund:adaboost}, which will be explained for completeness. Suppose we are choosing the $m^{th}$ weak classifier. Each training example $i$, a pixel in a training image, is paired with a target value $z_i^c \in \{-1, +1\}$---where $+1$ means that pixel $i$ has ground truth class $c$ and $-1$ not---and assigned a weight $w_i^c$ specifying its classification accuracy for class $c$ after the $m-1$ previous rounds of boosting. The $m^{th}$ weak classifier is chosen by minimizing an error function $J_{error}$ weighted by $w_i^c$:
\begin{equation} 
J_{error} = \sum_c \sum_i w_i^c(z_i^c-h_i^m(c))^2
\end{equation}

The training examples are then re-weighted
\begin{equation} 
w_i^c := w_i^c e^{-z_i^c h_i^m(c)}
\end{equation}

Minimizing the error function $J_{error}$ requires, for each new weak classifier, an expensive brute-force search over the possible sharing classes in $C$, features $(r, t)$, and thresholds $\theta$. As shown in~\cite{jamie:texton_boost} however, given these parameters, a closed form solution does exist for $a$, $b$ and $k_c$.

\section{Application to road scenes: Sturgess et al.} \label{sec:sturgess}

In the more specific field of road scene segmentation, Sturgess et al.~\cite{sturgess:road_scene} have recently quite successfully segmented inner-city road scenes in 11 different classes. Their method builds on the work of Shotton et al. (see Section~\ref{sec:TextonBoost}) and on that of Brostow et al.~\cite{brostow:structure_from_motion} integrating the appearance-based features from TextonBoost with the structure-from-motion features from Brostow et al. (see Section~\ref{subsec:3d_features}) in a higher order CRF. According to the authors, the use of higher-order cliques---that is, cliques with several pixels, instead of only pairs of pixels like in TextonBoost---produces accurate segmentations with precise object boundaries. Figure~\ref{fig:sturgess_higher_order_cliques} shows how Sturgess et al. use an unsupervised meanshift segmentation of the input image to obtain regions that are used as higher-oder cliques and included in the energy function $U$ to be minimized.

\begin{figure}[htb]
\centering{
\includegraphics[scale=0.5,bb=0 0 737 189]{figures/sturgess_higher_order_cliques.png}
}
\caption[Sturgess: higher-order cliques]{The original image (left), its ground truth
labelling (centre) and the meanshift segmentation of the image (right). The segments in the meanshift segmentation on the right are used to define higher-order potentials, allowing for more precise object boundaries in the final segmentation.}
\label{fig:sturgess_higher_order_cliques}
\end{figure}

Sturgess et al. achieved an overall accuracy of $84\%$ compared to the previous state-of-the-art accuracy of $69\%$~\cite{brostow:structure_from_motion} on the challenging CamVid database~\cite{brostow:camvid}. The work of Sturgess is therefore especially important for this thesis as it successfully tackles the same inner-city scene segmentation problem. The CamVid database will be better described in chapter 5, where the results obtained by our implementation are compared with those of Sturgess et al.~\cite{sturgess:road_scene}.